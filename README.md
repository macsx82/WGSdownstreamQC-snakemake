# Post variant calling QC of WGS pipeline

The pipeline will perform some QC steps on a complete WGS dataset, on both variants and samples.

Variant related QC and metrics:

- [x] HWE
- [x] Heterozygosity rate
- [x] Call rate
- [x] Allele Frequency comparison with outbred populations

For a subset of individuals (all the INGI-FVG samples) also the following information were extracted:

- [x] Allele Frequency comparison with available snp array data
- [x] Non Reference Discordance comparison with available snp array data


Sample related:

- [x] Singleton distribution
- [x] Missingness
- [x] Coverage
- [x] Heterozygosity rate
- [x] PCA
- [x] PCA with 1000G

The pipeline will generate lists of samples or sites flagged for removal to generate the final dataset.
Only sites exceeding the user specified thresholds for HWE pvalue and missing rate will be removed automatically.


The pipeline will generate the following plots, to help data interpretation:

- [x] Singletons vs NRD
- [x] Allele Frequency comparison with outbred pop (all variants)
- [x] Allele Frequency comparison with outbred pop (most diverse variants)
- [x] Allele Frequency comparison with SNP array data (all variants)
- [x] Allele Frequency comparison with SNP array data (most diverse variants)
- [x] Read Depth (DP) vs Singletons
- [x] Het Rate distribution by sample
- [x] PCA
- [x] PCA with 1000G



The final data release has to be generated by hand, excluding samples and sites following the evaluation of the generated summaries (tables and plots).

---

## Setting things up

In order to run the pipeline, there are some requirements to fullfill and some set up needs to be perfomed.
In this current version, the pipeline is tested and configured to be run on the [ORFEO cluster](https://orfeo-documentation.readthedocs.io/en/latest/) .
It is possible to run the pipeline on the Apollo cluster, but it will require to manually specify the location of all software binaries in the provided config file.

### Required Software

The following software has to be installed system-wide, in a user-defined Conda environment or using the modules architecture (ORFEO cluster).

+ awk
+ sed
+ python3
+ [bcftools](http://www.htslib.org/doc/)
+ [vcftools](https://vcftools.github.io/man_latest.html)
+ [plink](https://www.cog-genomics.org/plink/1.9/)
+ [king](https://www.kingrelatedness.com/)
+ R
+ git
+ snakemake

At the moment, the load directive of each module on the ORFEO cluster is hard-coded in the pipeline code, with the form "module-name/version". Modules and versions used are:

+ bcftools/1.14
+ vcftools/0.1.16
+ plink/1.90
+ R/4.0.3

**Before switching to a new version of each software/module, a test run should be performed to check that the expected output files are generated and that they are consistent with the previous production version.**

### Required python packages

In order to run the pipeline, the following python packages have to be installed in your conda environment or system wide:

+ errno
+ gzip 
+ io
+ multiprocessing
+ os
+ pandas
+ pathlib
+ psutil
+ re
+ sys
+ matplotlib
+ numpy
+ collections


### ORFEO/general set up
1. Install Snakemake via conda ([link](https://snakemake.readthedocs.io/en/stable/getting\_started/installation.html));
    ```bash
    conda create -c conda-forge -c bioconda -n snakemake snakemake pandas
    ```
2. Activate the environment you created
    ```bash
    conda activate snakemake
    ```

### Apollo set up

1. Add the global snakemake environment to your environment list:
    ```bash
    conda config --append envs_dirs /shared/software/conda/envs
    conda config --prepend envs_dirs ~/.conda/envs
    ```

2. Check that the environment is available to you (you should see an entry "snakemake_g" in the list)
    ```bash
    conda env list
    ```
3. Load the environment
    ```bash
    conda activate snakemake_g
    ```
---

